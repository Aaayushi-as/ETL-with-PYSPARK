# ETL with PySpark ğŸš€
A mid-level **data engineering project** that demonstrates building a production-style **ETL pipeline** using **PySpark, Apache Airflow, PostgreSQL, and Amazon Redshift**.  
The pipeline extracts transactional data from PostgreSQL, transforms it into a **star schema (dimensions + fact table)** using PySpark, and loads it into a warehouse for **analytics and BI use cases**.
---

## ğŸ“‚ Repository Structure

ETL-with-PYSPARK/
â”œ README.md
â”œ docker-compose.yml # Lightweight Postgres setup
â”œ dags/
â”‚ â”” dvdrental_etl_dag.py                    # Airflow DAG: dims â†’ fact orchestration
â”œ spark_apps/ # PySpark ETL scripts
â”‚ â”œ load_dim_customer.py
â”‚ â”œ load_dim_movie.py
â”‚ â”œ load_dim_store.py
â”‚ â”œ load_dim_staff.py
â”‚ â”œ load_fact_sales.py
â”‚ â”” postgresql-42.5.1.jar                  # JDBC driver
â”œ sql_scripts/                             # Schema definitions
â”‚ â”œ create_dim_customer.sql
â”‚ â”œ create_dim_movie.sql
â”‚ â”œ create_dim_store.sql
â”‚ â”œ create_dim_staff.sql
â”‚ â”” create_fact_sales.sql
â”” config/
â”” credentials_template.json              # Template for DB credentials



---

## âš™ï¸ Tech Stack

- **PySpark** â€“ scalable ETL (Extract, Transform, Load)  
- **Apache Airflow** â€“ workflow scheduling & orchestration  
- **PostgreSQL** â€“ source database (dvdrental dataset)  
- **Amazon Redshift** â€“ target data warehouse (cloud-ready)  
- **SQL** â€“ schema creation & queries  
- **Docker** â€“ containerized PostgreSQL instance  

---

## ğŸ—ï¸ Architecture

**Workflow:**
1. **Extract** â€“ Data is pulled from PostgreSQL with JDBC.  
2. **Transform** â€“ PySpark scripts process raw tables into clean **dimensions** (`customer`, `movie`, `store`, `staff`,`sales`) and a **fact table** (`sales`).  
3. **Load** â€“ Star schema tables are written into Amazon Redshift (or local Postgres as a warehouse).  
4. **Orchestrate** â€“ Airflow DAG manages execution order: **dimensions â†’ fact**.  

---


---

## Pipeline Logic
1. **Initialization**:  
   - Airflow DAG (`dvdrental_etl_dag.py`) initializes and defines the ETL workflow.  
   - PySpark session starts for distributed processing.  

2. **Extraction**:  
   - Source data is extracted from the PostgreSQL **dvdrental** database using JDBC.  

3. **Transformation**:  
   - PySpark scripts (`spark_apps/`) clean, join, and reshape the extracted data.  
   - Data is modeled into **dimensions** (`customer`, `movie`, `store`, `staff`) and a **fact table** (`sales`).  

4. **Loading**:  
   - Transformed star schema tables are loaded into Amazon Redshift (or PostgreSQL warehouse).  

5. **Orchestration**:  
   - Airflow schedules tasks in the correct order (dimension tables first, then fact table).  

---

## Problems Faced
- **Schema Alignment**: Ensuring dimension and fact scripts had consistent keys for joins.  
- **Connection Handling**: Managing PostgreSQL and Redshift connectivity through JDBC drivers.  
- **Workflow Failures**: Debugging Airflow DAG dependencies and retries.  
- **Large Data Volumes**: Optimizing PySpark jobs by partitioning and caching intermediate datasets.  

---








